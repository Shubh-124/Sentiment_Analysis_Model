{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11fafa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading movie_reviews: <urlopen error [WinError\n",
      "[nltk_data]     10060] A connection attempt failed because the\n",
      "[nltk_data]     connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f51d54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_reviews.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc794b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'happy', 'bastard', \"'\", 's', 'quick', 'movie', ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing movie review of particular file\n",
    "movie_reviews.words(movie_reviews.fileids()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e0bc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing review with categories\n",
    "documents=[]\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((movie_reviews.words(fileid),category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf23ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with', 'more', 'and', 'more', 'television', 'shows', ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(documents)\n",
    "documents[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13fb388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "# Adding stopwords and punctuations\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "\n",
    "punctuations=list(string.punctuation)\n",
    "stops=(stopwords.words('english'))\n",
    "stops=stops+punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d89744ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10060] A connection attempt failed because\n",
      "[nltk_data]     the connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "# New way of creating POS_TAG\n",
    "\n",
    "from nltk.tag.api import TaggerI\n",
    "from nltk.tag.util import str2tuple, tuple2str, untag\n",
    "from nltk.tag.sequential import (\n",
    "    SequentialBackoffTagger,\n",
    "    ContextTagger,\n",
    "    DefaultTagger,\n",
    "    NgramTagger,\n",
    "    UnigramTagger,\n",
    "    BigramTagger,\n",
    "    TrigramTagger,\n",
    "    AffixTagger,\n",
    "    RegexpTagger,\n",
    "    ClassifierBasedTagger,\n",
    "    ClassifierBasedPOSTagger,\n",
    ")\n",
    "from nltk.tag.brill import BrillTagger\n",
    "from nltk.tag.brill_trainer import BrillTaggerTrainer\n",
    "from nltk.tag.tnt import TnT\n",
    "from nltk.tag.hunpos import HunposTagger\n",
    "from nltk.tag.stanford import StanfordTagger, StanfordPOSTagger, StanfordNERTagger\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger, HiddenMarkovModelTrainer\n",
    "from nltk.tag.senna import SennaTagger, SennaChunkTagger, SennaNERTagger\n",
    "from nltk.tag.mapping import tagset_mapping, map_tag\n",
    "from nltk.tag.crf import CRFTagger\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "from nltk.data import load, find\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "RUS_PICKLE = (\n",
    "    \"taggers/averaged_perceptron_tagger_ru/averaged_perceptron_tagger_ru.pickle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b03b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_tagger(lang):\n",
    "    if lang == \"rus\":\n",
    "        tagger = PerceptronTagger(False)\n",
    "        ap_russian_model_loc = \"file:\" + str(find(RUS_PICKLE))\n",
    "        tagger.load(ap_russian_model_loc)\n",
    "    else:\n",
    "        tagger = PerceptronTagger()\n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df7f29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _pos_tag(tokens, tagset=None, tagger=None, lang=None):\n",
    "    # Currently only supports English and Russian.\n",
    "    if lang not in [\"eng\", \"rus\"]:\n",
    "        raise NotImplementedError(\n",
    "            \"Currently, NLTK pos_tag only supports English and Russian \"\n",
    "            \"(i.e. lang='eng' or lang='rus')\"\n",
    "        )\n",
    "    # Throws Error if tokens is of string type\n",
    "    elif isinstance(tokens, str):\n",
    "        raise TypeError(\"tokens: expected a list of strings, got a string\")\n",
    "\n",
    "    else:\n",
    "        tagged_tokens = tagger.tag(tokens)\n",
    "        if tagset:  # Maps to the specified tagset.\n",
    "            if lang == \"eng\":\n",
    "                tagged_tokens = [\n",
    "                    (token, map_tag(\"en-ptb\", tagset, tag))\n",
    "                    for (token, tag) in tagged_tokens\n",
    "                ]\n",
    "            elif lang == \"rus\":\n",
    "                # Note that the new Russian pos tags from the model contains suffixes,\n",
    "                # see https://github.com/nltk/nltk/issues/2151#issuecomment-430709018\n",
    "                tagged_tokens = [\n",
    "                    (token, map_tag(\"ru-rnc-new\", tagset, tag.partition(\"=\")[0]))\n",
    "                    for (token, tag) in tagged_tokens\n",
    "                ]\n",
    "        return tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24d64c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(tokens, tagset=None, lang=\"eng\"):\n",
    "    tagger = _get_tagger(lang)\n",
    "    return _pos_tag(tokens, tagset, tagger, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "099190c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "# from nltk import pos_tag\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# converting pos_tag to lemmatizer pos\n",
    "def make_pos_simple(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb852226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Function:- Data Cleaning-Removing Stop words and Lemmatisation\n",
    "def remove_stop(sentence):\n",
    "    output=[]\n",
    "    for word in sentence:\n",
    "        if word.lower() not in stops:\n",
    "            # word should be passed as an array into pos tag\n",
    "            pos=pos_tag([word])\n",
    "            # array(word,pos)\n",
    "            clean_word=lemmatizer.lemmatize(word,pos=make_pos_simple(pos[0][1]))\n",
    "            output.append(clean_word.lower())\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b22c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lammetizing top 10 documents\n",
    "documents=[(remove_stop(document),category) for document,category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a77bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_doc=documents[0:1500]\n",
    "testing_doc=documents[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca44d8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# building feature set\n",
    "# taking the mostly repeated words as feature(frequency distribution)\n",
    "all_words=[]\n",
    "for doc in training_doc:\n",
    "    all_words+=doc[0]\n",
    "import nltk\n",
    "freq=nltk.FreqDist(all_words)\n",
    "common=freq.most_common(1000)\n",
    "# Storing most common words as features \n",
    "features=[i[0] for i in common]\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faabbcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check if the most common features are present in particular document or not\n",
    "def get_common(words):\n",
    "    current_features={}\n",
    "    words_set=set(words)\n",
    "    for w in features:\n",
    "        current_features[w]=w in words_set\n",
    "    return current_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7beacd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=[(get_common(doc),category) for doc,category in training_doc]\n",
    "testing_data=[(get_common(doc),category) for doc,category in testing_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fda42d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Naive Bayes Classifier to Train\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier=NaiveBayesClassifier.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f5fd2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.742"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier,testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d3464ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   waste = True              neg : pos    =      5.8 : 1.0\n",
      "              ridiculous = True              neg : pos    =      5.6 : 1.0\n",
      "                   awful = True              neg : pos    =      4.9 : 1.0\n",
      "                  stupid = True              neg : pos    =      4.4 : 1.0\n",
      "                  boring = True              neg : pos    =      4.0 : 1.0\n",
      "                terrible = True              neg : pos    =      3.7 : 1.0\n",
      "               realistic = True              pos : neg    =      3.6 : 1.0\n",
      "                    mess = True              neg : pos    =      3.4 : 1.0\n",
      "               excellent = True              pos : neg    =      3.4 : 1.0\n",
      "               memorable = True              pos : neg    =      3.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
